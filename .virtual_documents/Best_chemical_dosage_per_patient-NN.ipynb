





import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import time as time
valid_IC50s = pd.read_csv("valid_IC50s_within_range.csv")
merged_df = pd.read_csv("final_merged.csv")
lucas_df = pd.read_csv("final_mapping.csv")
cell_lines_df = pd.read_csv("data/HarvardCellLines.csv")


valid_IC50s.drop(columns = ['Unnamed: 0', 'N Points'], inplace = True)


cell_lines_df.columns


columns = ["HMS LINCS ID", "Name"]
cell_lines_df = cell_lines_df[columns]


cell_lines_df_dict = cell_lines_df.set_index("HMS LINCS ID").to_dict()["Name"]
cell_lines_df_dict


def cell_name_map(row, dictionary):
    return dictionary[row["Cell_Line"]]


merged_df["Race"].value_counts()


# # TEMPORARY IMPUTATION OF RACE BASED OFF PROPORTIONS OF EXISTING RACES.
# # WILL USE AUSTIN'S IMPUTED RACE ANALYSIS LATER.

# # Get value counts as probabilities
# race_dist = merged_df['Race'].value_counts(normalize=True)

# # Get the indices where race is missing
# missing_indices = lucas_df['Cell_Line_Race'].isna()

# # Sample values based on observed distribution
# imputed_values = np.random.choice(race_dist.index, size=missing_indices.sum(), p=race_dist.values)

# # Assign the sampled values to the missing positions
# lucas_df.loc[missing_indices, 'Cell_Line_Race'] = imputed_values



# TEMPORARY IMPUTATION OF RACE BASED OFF PROPORTIONS OF EXISTING RACES.
# WILL USE AUSTIN'S IMPUTED RACE ANALYSIS LATER.

# Get value counts as probabilities
race_dist = merged_df['Race'].value_counts(normalize=True)

# Get the indices where race is missing
missing_indices = merged_df['Race'].isna()

# Sample values based on observed distribution
imputed_values = np.random.choice(race_dist.index, size=missing_indices.sum(), p=race_dist.values)

# Assign the sampled values to the missing positions
merged_df.loc[missing_indices, 'Race'] = imputed_values


def T_stage_by_size(size):
    if size == 0:
        return 0
    if size > 0 and size <= 20:
        return 1
    if size > 20 and size <= 50:
        return 2
    if size > 50:
        return 3


merged_df['T_stage_by_size'] = merged_df.apply(lambda row: row['T Stage'] if pd.notnull(row['T Stage']) else T_stage_by_size(row['Tumor Size']), axis=1)


merged_df['T_stage_by_size'].isna().sum()


columns = ['Patient ID', 'Age', 'Race', 'T_stage_by_size']
patients_df = merged_df[columns]


patients_df


patients_df.isna().sum()


valid_IC50s


# ASK LUCAS FOR THIS DATA
patients_df = patients_df.copy()

# Assign a random integer between 1 and 34 for each row
patients_df['cell_line'] = np.random.randint(1, 36, size=len(patients_df))


patients_df





# ASK TEAM TO HELP IMPUTE MISSING T-STAGE OR DROP THEIR ROWS 
# MAYBE ASK AUSTIN FOR SIMILAR WORKFLOW USED FOR RACE BUT FOR T-STAGE 
patients_df.isna().sum()





valid_IC50s


# Get unique cell line names
unique_cell_lines = valid_IC50s['Cell Name'].unique()

# Create a mapping from cell line name to number (1 to 34)
cell_line_map = {name: i+1 for i, name in enumerate(unique_cell_lines)}

print(cell_line_map)

valid_IC50s['Cell_Name_Mapped'] = valid_IC50s['Cell Name'].map(cell_line_map)


valid_IC50s


# One-hot encode 'Small Molecule Name'
one_hot_encoded = pd.get_dummies(valid_IC50s['Small Molecule Name'], dtype='int')

# Concatenate the one-hot encoded columns back to the original DataFrame
valid_IC50s_encoded = pd.concat([valid_IC50s, one_hot_encoded], axis=1)


valid_IC50s_encoded


patients_df


valid_IC50s


# Make sure both dataframes have a matching cell line column
# Map Cell Name (string) to cell_line_id (integer) in the drug data
# valid_IC50s_encoded['cell_line_id'] = valid_IC50s_encoded['Cell Name'].map(cell_line_map)

# Merge patients with drug data on cell_line == cell_line_id
# This will replicate each patient row once per matching drug for their cell line
patient_drug_df = patients_df.merge(
    valid_IC50s,
    left_on='cell_line',
    right_on='Cell_Name_Mapped',
    how='inner'  # use 'inner' to only keep rows with valid matches
)


patient_drug_df.drop(columns="cell_line", inplace=True)


race_dict = {0:'N/A', 1:"white", 2:"black", 3:"asian", 4:"native", 5:"hispanic", 6:"multi", 7:"hawa", 8:"amer indian"}


patient_drug_df


SmallMolecules = pd.read_csv("data/HarvardSmallMolecules.csv")
columns = ['Name', 'Molecular Mass', 'SMILES']
SmallMolecules = SmallMolecules[columns]
SmallMolecules


SMILES_Map = SmallMolecules[["Name", "SMILES"]].set_index("Name").to_dict()['SMILES']


SMILES_Map


def map_smiles(row, dictionary):
    return dictionary[row["Small Molecule Name"]]


patient_drug_df["SMILES"] = patient_drug_df.apply(map_smiles,dictionary = SMILES_Map,axis=1)


patient_drug_df


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# Separate features (X) and target (y)
X = patient_drug_df.drop(columns=['Patient ID', 'EC50 (uM)', 'Small Molecule Name', 'Cell_Name_Mapped'])  # Drop EC50 and non-features
y = patient_drug_df['EC50 (uM)']  # EC50 is the target

# Standardize numerical features
scaler = StandardScaler()
X[['Age']] = scaler.fit_transform(X[['Age']]) 

# Need to one-hot encode categorical variables: Race, Drug name, Cell line name
X = pd.get_dummies(X, columns=['Cell Name', 'Race', 'T_stage_by_size'],dtype='int', drop_first=False)


# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


X_train


import re
# MAYBE SWITCH THIS OUT WITH CHEMBERTA/TRANSFORMER FOR TOKENIZATION 
def tokenize_smiles(smiles):
    """
    Tokenizes a SMILES string into a list of atomic or multi-char tokens.
    Handles 'Cl', 'Br', 'nH', etc.
    """
    pattern = r"(\[[^\[\]]{1,6}\]|Br|Cl|Si|Se|@@?|=|#|\(|\)|\.|\/|\\|\+|\-|\d+|[A-Z][a-z]?|[a-z])"
    return re.findall(pattern, smiles)


import torch
import torch.nn as nn
import torch.nn.functional as F

# OTHER ALTERNATIVE FOR TOKENIZATION 

def build_vocab(smiles_list):
    tokens = sorted(set(char for sm in smiles_list for char in tokenize_smiles(sm)))
    token_to_idx = {token: i + 1 for i, token in enumerate(tokens)}  # 0 = padding
    return token_to_idx

# Convert SMILES to indices
def smiles_to_indices(smiles, token_to_idx, max_len=120):
    tokens = tokenize_smiles(smiles)
    token_ids = [token_to_idx.get(tok, 0) for tok in tokens][:max_len]
    padded = token_ids + [0] * (max_len - len(token_ids))
    return padded



# TWO NEURAL NETS: CNN and FEED FORWARD NEURAL NETWORK

# CNN 
class SMILESEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim=18, out_channels=32, kernel_size=3):
        super(SMILESEncoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=out_channels, kernel_size=kernel_size, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveMaxPool1d(1)  # Get fixed-size output
     
    def forward(self, x):
        x = self.embedding(x)      
        x = x.permute(0, 2, 1)        
        x = self.relu(self.conv1(x))  
        x = self.pool(x)              
        x = x.squeeze(-1)             
        return x

# SIMGPLE FEED FORWARD NEURAL NETWORK 
class PatientEncoder(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 32)
        )

    def forward(self, x):
        return self.encoder(x.float())

# REGRESSION PREDICTION WITH SIMPLE FEED FORWARD NEURAL NETWORK

class DrugResponsePredictor(nn.Module):
    def __init__(self, vocab_size, patient_input_dim):
        super().__init__()
        self.smiles_encoder = SMILESEncoder(vocab_size)
        self.patient_encoder = PatientEncoder(patient_input_dim)
        self.fc = nn.Sequential(
            nn.Linear(32 + 32, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # IC50 prediction
        )

    def forward(self, smiles_seq, patient_feat):
        drug_vec = self.smiles_encoder(smiles_seq)
        patient_vec = self.patient_encoder(patient_feat)
        combined = torch.cat([drug_vec, patient_vec], dim=1)
        return self.fc(combined).squeeze(-1)



from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import AllChem
from rdkit.Chem import DataStructs
from IPython.display import display, HTML


X_train


y_train


# Data
smiles_list = X_train["SMILES"]  # list of SMILES strings
patient_features = X_train.drop(columns="SMILES").to_numpy()  # (n_samples, patient_dim)
ic50_values = torch.tensor(y_train.to_numpy()).float()  # target IC50

token_to_idx = build_vocab(smiles_list)
smiles_tensor = torch.tensor([smiles_to_indices(sm, token_to_idx) for sm in smiles_list])
patient_tensor = torch.tensor(patient_features).float()
ic50_tensor = ic50_values# torch.tensor(ic50_values).float()

# Model
model = DrugResponsePredictor(vocab_size=len(token_to_idx), patient_input_dim=patient_tensor.shape[1])
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

# Training loop
model.train()
start_time = time.time() 
for epoch in range(1):
    optimizer.zero_grad()
    preds = model(smiles_tensor, patient_tensor)
    loss = loss_fn(preds, ic50_tensor)
    loss.backward()
    optimizer.step()
    end_time = time.time()
    elapsed = end_time - start_time
    print(f"Epoch {epoch} | Loss: {loss.item():.4f}")
    print(f"Time elapsed: {elapsed} seconds")



import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(model, smiles_list, patient_features, true_ic50s, token_to_idx):
    # Tokenize SMILES
    smiles_indices = [smiles_to_indices(sm, token_to_idx) for sm in smiles_list]
    smiles_tensor = torch.tensor(smiles_indices)

    patient_features = patient_features.to_numpy()
    true_ic50s = true_ic50s.to_numpy()
    
    # Prepare patient features and true labels
    patient_tensor = torch.tensor(patient_features).float()
    true_ic50_tensor = torch.tensor(true_ic50s).float()
    
    with torch.no_grad():
        preds = model(smiles_tensor, patient_tensor).cpu().numpy()
        targets = true_ic50_tensor.cpu().numpy()
    
    mse = mean_squared_error(targets, preds)
    mae = mean_absolute_error(targets, preds)
    r2 = r2_score(targets, preds)
    
    return mse, mae, r2,
    


metrics = evaluate_model(model, X_test["SMILES"], X_test.drop(columns="SMILES"), y_test, token_to_idx)
print(f"mse {metrics[0]}")
print(f"mae {metrics[1]}")
print(f"r2 {metrics[2]}")


def recommend_top_k_drugs_dual_encoder(model, patient_feat, all_smiles, token_to_idx, k=5):
    model.eval()

    # Tokenize SMILES
    smiles_indices = [smiles_to_indices(sm, token_to_idx) for sm in all_smiles]
    smiles_tensor = torch.tensor(smiles_indices)

    # Repeat patient features for all drugs
    patient_tensor = torch.tensor(patient_feat).float().unsqueeze(0).repeat(len(all_smiles), 1)

    # Get predictions
    with torch.no_grad():
        preds = model(smiles_tensor, patient_tensor)

    # Sort & select top-k drugs
    topk = torch.topk(preds, k=k, largest=False)
    topk_indices = topk.indices
    topk_ic50s = topk.values

    # Return SMILES + predicted IC50s
    recommendations = []
    for idx, ic50 in zip(topk_indices, topk_ic50s):
        recommendations.append({
            "smiles": all_smiles[idx],
            "predicted_ic50": ic50.item()
        })

    return recommendations






X


# Recommend for a given patient
patient_i = X.drop(columns="SMILES").iloc[80].to_numpy()
top5 = recommend_top_k_drugs_dual_encoder(model, patient_i, smiles_list.tolist(), token_to_idx)


top5






